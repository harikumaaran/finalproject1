{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport re\nimport pickle\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:28.582020Z","iopub.execute_input":"2021-10-07T13:40:28.582716Z","iopub.status.idle":"2021-10-07T13:40:33.014868Z","shell.execute_reply.started":"2021-10-07T13:40:28.582627Z","shell.execute_reply":"2021-10-07T13:40:33.013585Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nINPUTS -> Encoder -> ENC OUTPUTS, THOUGHT VECTOR -> \n\nAttention Network -> Attention Weights (x ENC OUTPUTS) -> ATTENTION OUTPUT\n\nATTENTION OUTPUT, ACTUAL OUTPUT(INPUT) -> DECODER -> FINAL OUTPUT\n\"\"\"\n\n\"\"\"\nENCODER ARCHITECTURE:-\n\n\nINPUTS -> EMBEDDING -> GRU\n\"\"\"\n\n\"\"\"\nATTENTION NETWORK ARCHITECTURE\n\nENC OUTPUTS     -> ENC LAYER     -> --------       \n                                            ------> ACTIVATION -> FINAL LAYER -> ATTENTION WEIGHTS\nTHOUGHT VECTOR  -> THOUGHT LAYER -> --------\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.016592Z","iopub.execute_input":"2021-10-07T13:40:33.016847Z","iopub.status.idle":"2021-10-07T13:40:33.034729Z","shell.execute_reply.started":"2021-10-07T13:40:33.016813Z","shell.execute_reply":"2021-10-07T13:40:33.033189Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n        super(Encoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.enc_units = encoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n    \n    def call(self, inputs, hidden_state):\n        embedded_inputs = self.embedding(inputs)\n        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n        return enc_outputs, thought_vector","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.036119Z","iopub.execute_input":"2021-10-07T13:40:33.036715Z","iopub.status.idle":"2021-10-07T13:40:33.049775Z","shell.execute_reply.started":"2021-10-07T13:40:33.036678Z","shell.execute_reply":"2021-10-07T13:40:33.048818Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        \n        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, enc_outputs, thought_vector):\n        thought_matrix = tf.expand_dims(thought_vector, 1)\n        \n        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n        \n        attention_output = attention_weights * enc_outputs # Shape (batch_size, num_outputs, output_size)\n        attention_output = tf.reduce_sum(attention_output, axis=1) # New shape (batch_size, output_size)\n        \n        return attention_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.054821Z","iopub.execute_input":"2021-10-07T13:40:33.055039Z","iopub.status.idle":"2021-10-07T13:40:33.071723Z","shell.execute_reply.started":"2021-10-07T13:40:33.055015Z","shell.execute_reply":"2021-10-07T13:40:33.070483Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n        super(Decoder, self).__init__()\n        \n        self.batch_size = batch_size\n        self.dec_units = decoder_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n        self.attention = Attention(self.dec_units)\n        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n        \n    def call(self, inputs, enc_outputs, thought_vector):\n        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n        #Shape of attention output (batch_size, size_of_embedding)\n        \n        embedded_inputs = self.embedding(inputs) #Shape (batch_size, num_words, size_of_embedding)\n        attention_output = tf.expand_dims(attention_output, 1) #Shape of attention output (batch_size, 1, size_of_embedding)\n        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n        \n        decoder_outputs, hidden_state = self.gru(concat_inputs) #Shape (batch_size, 1, size_of_embedding)\n        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) #Shape (batch_size, size_of_embedding)\n        \n        final_outputs = self.word_output(decoder_outputs)\n        return final_outputs, hidden_state, attention_weights","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.074371Z","iopub.execute_input":"2021-10-07T13:40:33.075076Z","iopub.status.idle":"2021-10-07T13:40:33.095314Z","shell.execute_reply.started":"2021-10-07T13:40:33.075011Z","shell.execute_reply":"2021-10-07T13:40:33.094250Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Train:\n    def __init__(self):\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        \n    def loss_function(self, y_real, y_pred):\n        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n        base_loss = self.base_loss_function(y_real, y_pred)\n        \n        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n        final_loss = mask * base_loss\n        \n        return tf.reduce_mean(final_loss)\n    \n    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n        loss = 0\n        \n        with tf.GradientTape() as tape:\n            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n            dec_hidden = thought_vector\n            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n            \n            for index in range(1, label_data.shape[1]):\n                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n                \n                dec_input = tf.expand_dims(label_data[:, index], 1)\n                loss = loss + self.loss_function(label_data[:, index], outputs)\n        \n        word_loss = loss / int(label_data.shape[1])\n        \n        variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, variables)\n        self.optimizer.apply_gradients(zip(gradients, variables))\n        \n        return word_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.100926Z","iopub.execute_input":"2021-10-07T13:40:33.101406Z","iopub.status.idle":"2021-10-07T13:40:33.121285Z","shell.execute_reply.started":"2021-10-07T13:40:33.101370Z","shell.execute_reply":"2021-10-07T13:40:33.120362Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n[he she it name this that these those their you]\n\ny_real = [0, 0, 0, 0, '1', 0, 0, 0, 0, 0]\n\nmath.equal() = [True, True, True, True, False, True, True, True, True, True]\nlogical not = \nmask = [1, 1, 1, 1, '0', 1, 1, 1, 1, 1]\n\n[0.001, 0.001, 0.001, 0.001, '0.9', 0.001, 0.001, 0.001, 0.001, 0.002]\n\n\n[1, 1, 1, 1, '0', 1, 1, 1, 1, 1] * [0.001, 0.001, 0.001, 0.001, '-0.1', 0.001, 0.001, 0.001, 0.001, 0.002]\n\nfinal_loss = [0.001, 0.001, 0.001, 0.001, 0, 0.001, 0.001, 0.001, 0.001, 0.002]\n\nreturn 0.0013\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.125944Z","iopub.execute_input":"2021-10-07T13:40:33.129083Z","iopub.status.idle":"2021-10-07T13:40:33.140341Z","shell.execute_reply.started":"2021-10-07T13:40:33.129045Z","shell.execute_reply":"2021-10-07T13:40:33.139372Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Data_Preprocessing:\n    def __init__(self):\n        self.temp = None\n    \n    def get_data(self, path):\n        file = open(path, 'r').read()\n        lists = [f.split('\\t') for f in file.split('\\n')]\n        \n        questions = [x[0] for x in lists]\n        answers = [x[1] for x in lists]\n        \n        return questions, answers\n    \n    def process_sentence(self, line):\n        line = line.lower().strip()\n        \n        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n        line = re.sub(r'[\" \"]+', \" \", line)\n        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n        line = line.strip()\n        \n        line = '<start> ' + line + ' <end>'\n        return line\n    \n    def word_to_vec(self, inputs):\n        tokenizer = Tokenizer(filters='')\n        tokenizer.fit_on_texts(inputs)\n        \n        vectors = tokenizer.texts_to_sequences(inputs)\n        vectors = pad_sequences(vectors, padding='post')\n        \n        return vectors, tokenizer","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.145106Z","iopub.execute_input":"2021-10-07T13:40:33.147802Z","iopub.status.idle":"2021-10-07T13:40:33.161327Z","shell.execute_reply.started":"2021-10-07T13:40:33.147769Z","shell.execute_reply":"2021-10-07T13:40:33.160439Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data = Data_Preprocessing()\n\nquestions, answers = data.get_data('../input/shapeai-chatbot/chatbot.txt')\n\nquestions = [data.process_sentence(str(sentence)) for sentence in questions]\nanswers = [data.process_sentence(str(sentence)) for sentence in answers]\n\ntrain_vectors, train_tokenizer = data.word_to_vec(questions)\nlabel_vectors, label_tokenizer = data.word_to_vec(answers)\n\nmax_length_train = train_vectors.shape[1]\nmax_length_label = label_vectors.shape[1]\n\nbatch_size = 64\nbuffer_size = train_vectors.shape[0]\nembedding_dim = 256\nsteps_per_epoch = buffer_size//batch_size\nunits = 1024","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.165714Z","iopub.execute_input":"2021-10-07T13:40:33.168588Z","iopub.status.idle":"2021-10-07T13:40:33.725204Z","shell.execute_reply.started":"2021-10-07T13:40:33.168551Z","shell.execute_reply":"2021-10-07T13:40:33.724510Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"vocab_train = len(train_tokenizer.word_index) + 1\nvocab_label = len(label_tokenizer.word_index) + 1","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.728693Z","iopub.execute_input":"2021-10-07T13:40:33.728956Z","iopub.status.idle":"2021-10-07T13:40:33.734172Z","shell.execute_reply.started":"2021-10-07T13:40:33.728926Z","shell.execute_reply":"2021-10-07T13:40:33.731711Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\ndataset = dataset.shuffle(buffer_size)\ndataset = dataset.batch(batch_size, drop_remainder=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:33.735728Z","iopub.execute_input":"2021-10-07T13:40:33.736119Z","iopub.status.idle":"2021-10-07T13:40:35.397109Z","shell.execute_reply.started":"2021-10-07T13:40:33.736084Z","shell.execute_reply":"2021-10-07T13:40:35.395614Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\ndecoder = Decoder(vocab_label, embedding_dim, units, batch_size)\ntrainer = Train()","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:35.398355Z","iopub.execute_input":"2021-10-07T13:40:35.398747Z","iopub.status.idle":"2021-10-07T13:40:35.703989Z","shell.execute_reply.started":"2021-10-07T13:40:35.398705Z","shell.execute_reply":"2021-10-07T13:40:35.703267Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\n\nfor epoch in range(1, EPOCHS + 1):\n    enc_hidden = tf.zeros((batch_size, units))\n    total_loss = 0\n    \n    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n        total_loss = total_loss + batch_loss\n        \n    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-07T13:40:35.705402Z","iopub.execute_input":"2021-10-07T13:40:35.705775Z","iopub.status.idle":"2021-10-07T13:49:08.172011Z","shell.execute_reply.started":"2021-10-07T13:40:35.705738Z","shell.execute_reply":"2021-10-07T13:49:08.171221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Chatbot:\n    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n        self.train_tokenizer = train_tokenizer\n        self.label_tokenizer = label_tokenizer\n        self.encoder = encoder\n        self.decoder = decoder\n        self.units = units\n        self.data = Data_Preprocessing()\n        self.maxlen = max_length_train\n    \n    def clean_answer(self, answer):\n        answer = answer[:-1]\n        answer = ' '.join(answer)\n        return answer\n    \n    def predict(self, sentence):\n        sentence = self.data.process_sentence(sentence)\n        \n        sentence_mat = []\n        for word in sentence.split(\" \"):\n            try:\n                sentence_mat.append(self.train_tokenizer.word_index[word])\n            except:\n                return \"Could not understand that, can you re-phrase?\"\n        \n        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n        sentence_mat = tf.convert_to_tensor(sentence_mat)\n        \n        enc_hidden = [tf.zeros((1, self.units))]\n        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n        \n        dec_hidden = thought_vector\n        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n        \n        answer = []\n        for i in range(1, self.maxlen):\n            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n            \n            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n            answer.append(word)\n            \n            if word == '<end>':\n                return self.clean_answer(answer)\n            \n            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n        \n        return self.clean_answer(answer)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:27:10.477265Z","iopub.execute_input":"2021-10-07T14:27:10.477549Z","iopub.status.idle":"2021-10-07T14:27:10.490139Z","shell.execute_reply.started":"2021-10-07T14:27:10.477515Z","shell.execute_reply":"2021-10-07T14:27:10.489280Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:27:10.737628Z","iopub.execute_input":"2021-10-07T14:27:10.738190Z","iopub.status.idle":"2021-10-07T14:27:10.742595Z","shell.execute_reply.started":"2021-10-07T14:27:10.738154Z","shell.execute_reply":"2021-10-07T14:27:10.741584Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"question = ''\nwhile True:\n    question = str(input('You:'))\n    if question == 'quit' or question == 'Quit':\n        break\n        \n    answer = bot.predict(question)\n    print(f'Bot: {answer}')","metadata":{"execution":{"iopub.status.busy":"2021-10-07T14:33:48.858766Z","iopub.execute_input":"2021-10-07T14:33:48.859295Z","iopub.status.idle":"2021-10-07T14:34:23.002604Z","shell.execute_reply.started":"2021-10-07T14:33:48.859260Z","shell.execute_reply":"2021-10-07T14:34:23.001711Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"pred = [[1,2,3,4,5]]\npred[0] = [1,2,3,4,5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence - english sentence\n\nremove things from it \nconvert it to one hot form\npass it through the whole model and get the predictions","metadata":{},"execution_count":null,"outputs":[]}]}